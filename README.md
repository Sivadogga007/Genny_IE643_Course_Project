I have used LORA adapter for finetuning the model with hyper parameters as rank=16,alpha=16,dropout=0.01
