{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it2NIBOfwmdi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47v1E1bUwhS2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDqzmM1HRApO",
        "outputId": "377939d5-7a87-4850-c915-f0d0af612b82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSON data has been saved to kaggle_data.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "data_dict = {\"username\":\"doggasivakumar\",\"key\":\"5865cfb60ae125784c1af8d78313993c\"}\n",
        "\n",
        "\n",
        "# Convert the dictionary to JSON\n",
        "json_data = json.dumps(data_dict, indent=4)  # Use indent for pretty printing\n",
        "\n",
        "\n",
        "# Option 1: Save the JSON to a file in the Colab environment\n",
        "with open('kaggle_data.json', 'w') as f:\n",
        "    f.write(json_data)\n",
        "\n",
        "# Option 2: Download the JSON file directly to your local machine\n",
        "# files.download('kaggle_data.json') # uncomment to download\n",
        "\n",
        "# Now you can use 'kaggle_data.json' for Kaggle competitions\n",
        "# You'll need to add it to your Kaggle dataset or use it in your notebook\n",
        "\n",
        "print(\"JSON data has been saved to kaggle_data.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5FQAKTtRpww",
        "outputId": "9a073f62-8b64-450d-eba3-61a1fa20bd7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 7, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 407, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n",
            "ls: cannot access '/content/downloaded_notebook': No such file or directory\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "bbqmfgJQvS4W",
        "outputId": "b323b4e0-c67c-426b-ed84-d6b6ce4a728d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-17da8afd-9358-4302-8720-559056c9d6e3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-17da8afd-9358-4302-8720-559056c9d6e3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0GcN1G-SJ4F",
        "outputId": "007a954c-0de1-4815-90d2-fedd53fcac81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output file downloaded to /content/model.pth\n",
            "Output file downloaded to /content/training_metrics.json\n",
            "Kernel log downloaded to /content/sharingbe.log \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Step 1: Define the Kaggle credentials dictionary\n",
        "kaggle_credentials = {\"username\":\"sivadogga\",\"key\":\"b56da0155f5e3e607c8cffaf048d686c\"}\n",
        "\n",
        "# Step 2: Save the dictionary as kaggle.json\n",
        "kaggle_json_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(kaggle_json_path), exist_ok=True)\n",
        "\n",
        "# Write the credentials to the file\n",
        "with open(kaggle_json_path, \"w\") as f:\n",
        "    json.dump(kaggle_credentials, f)\n",
        "\n",
        "# Set the correct permissions for security\n",
        "os.chmod(kaggle_json_path, 0o600)\n",
        "\n",
        "\n",
        "!kaggle kernels output sivadogga/sharingbe -p /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "dc42b33d2f774b1a8c9b160ce64b9d86",
            "ef3794f8875f4a7cb8b87650aa73ecdc",
            "51440bc411cb480f8bb2eca3239e1be1",
            "6abdf6f7d6004786a230528066745e27",
            "c3a58a0280414e08a77dba63d43dc8aa"
          ]
        },
        "id": "e_Xeijf0SMTE",
        "outputId": "c8df8ece-1bb3-438c-e426-c9f0e07cb16f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc42b33d2f774b1a8c9b160ce64b9d86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle credentials set.\n",
            "Kaggle credentials successfully validated.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n7xLe71SwxQ",
        "outputId": "5273617a-499e-477c-ff07-c36f7dac4292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/doggasivakumar/smallocr?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 539M/539M [00:30<00:00, 18.3MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "doggasivakumar_mathdocumazys_path = kagglehub.dataset_download('doggasivakumar/smallocr')\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43NlRCmBS_Zr",
        "outputId": "4d4cbd89-1ea9-49f9-ccce-8a21798ad958"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/root/.cache/kagglehub/datasets/doggasivakumar/modell/versions/1'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doggasivakumar_mathdocumazys_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdkOI6M3TEyv",
        "outputId": "6555f6dd-a932-435a-cfa2-ed28f0d761a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['checkpoint-6770']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir(\"/root/.cache/kagglehub/datasets/doggasivakumar/smallocr/versions/1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVKj4ibAT_r7",
        "outputId": "45133fd4-3dbc-431b-d25b-65b6eab324e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/doggasivakumar/dataset-model?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 344k/344k [00:00<00:00, 506kB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n",
            "Data source import complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "doggasivakumar_mathdocumazys_path = kagglehub.dataset_download('doggasivakumar/dataset-model')\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5NQwID6KUIgx",
        "outputId": "371bde9c-1454-4e83-8f76-2706164f683c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/root/.cache/kagglehub/datasets/doggasivakumar/dataset-model/versions/1'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doggasivakumar_mathdocumazys_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixDW1YzVVJx3",
        "outputId": "19cc5c0a-7c56-4f4a-e5c3-64ed1710aaa0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['mathematical_image.jpg', '20241107_185154.jpg']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir('/root/.cache/kagglehub/datasets/doggasivakumar/dataset-model/versions/1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QvTeTU9TiKs",
        "outputId": "da41ba8e-84f1-4772-b043-c26813d97b72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (11.0.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.2)\n",
            "Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, pdf2image, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.5 pdf2image-1.17.0 starlette-0.41.3 uvicorn-0.32.1\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi pdf2image uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4ac_v7xWDsm",
        "outputId": "7909de50-77c5-4700-ebd7-99e56c215f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.26.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.26.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading python_Levenshtein-0.26.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.26.1 python-Levenshtein-0.26.1 rapidfuzz-3.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I686M-CpTqc5",
        "outputId": "f943941d-4bf8-4ed2-887e-905e04cb6d13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package propeller\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!apt-get install propeller\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XlHNcW9W8n_"
      },
      "outputs": [],
      "source": [
        "!apt-get install texlive-latex-base\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6hrdhTlWi0q"
      },
      "outputs": [],
      "source": [
        "# prompt: create folder pdflatex\n",
        "\n",
        "!mkdir pdflatex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CpN8qah3BjJ",
        "outputId": "2eceaf7f-3425-4f13-ec91-cdd566d6974f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Config of the encoder: <class 'transformers.models.donut.modeling_donut_swin.DonutSwinModel'> is overwritten by shared encoder config: DonutSwinConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"depths\": [\n",
            "    2,\n",
            "    2,\n",
            "    14,\n",
            "    2\n",
            "  ],\n",
            "  \"drop_path_rate\": 0.1,\n",
            "  \"embed_dim\": 128,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"image_size\": [\n",
            "    896,\n",
            "    672\n",
            "  ],\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"mlp_ratio\": 4.0,\n",
            "  \"model_type\": \"donut-swin\",\n",
            "  \"num_channels\": 3,\n",
            "  \"num_heads\": [\n",
            "    4,\n",
            "    8,\n",
            "    16,\n",
            "    32\n",
            "  ],\n",
            "  \"num_layers\": 4,\n",
            "  \"patch_size\": 4,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.46.2\",\n",
            "  \"use_absolute_embeddings\": false,\n",
            "  \"window_size\": 7\n",
            "}\n",
            "\n",
            "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_cross_attention\": true,\n",
            "  \"add_final_layer_norm\": true,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 4,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_decoder\": true,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"max_position_embeddings\": 3584,\n",
            "  \"model_type\": \"mbart\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.46.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "<ipython-input-30-7838a08b6e41>:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  formulamodel.load_state_dict(torch.load(\"/content/model.pth\"))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from fastapi import FastAPI, File, UploadFile,BackgroundTasks\n",
        "from fastapi.responses import JSONResponse, FileResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import uvicorn\n",
        "from pdf2image import convert_from_path\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.responses import JSONResponse\n",
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from transformers import NougatProcessor, VisionEncoderDecoderModel\n",
        "import numpy as np\n",
        "max_length = 100 # defing max length of output\n",
        "\n",
        "import torch\n",
        "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "formulaprocessor = NougatProcessor.from_pretrained(r\"CuiSiwei/nougat-for-formula\", max_length = max_length) # Replace with your path\n",
        "\n",
        "formulamodel = VisionEncoderDecoderModel.from_pretrained(r\"CuiSiwei/nougat-for-formula\").to(device) # Replace with your path\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "\n",
        "    r=16,\n",
        "\n",
        "    lora_alpha=16,\n",
        "\n",
        "    # target_modules=[\n",
        "\n",
        "    #     # Encoder (Swin Transformer) modules\n",
        "\n",
        "    #     \"attn.qkv\",\n",
        "\n",
        "    #     \"attn.proj\",\n",
        "\n",
        "    #     \"mlp.fc1\",\n",
        "\n",
        "    #     \"mlp.fc2\",\n",
        "\n",
        "    #     # Decoder (GPT-2) modules\n",
        "\n",
        "    #     \"c_attn\",\n",
        "\n",
        "    #     \"c_proj\",\n",
        "\n",
        "    #     \"c_fc\",\n",
        "\n",
        "    #     \"attn.c_proj\",\n",
        "\n",
        "    # ],\n",
        "\n",
        "    target_modules=\"all-linear\",\n",
        "\n",
        "    lora_dropout=0.1,\n",
        "\n",
        "    bias=\"none\"\n",
        "\n",
        "    # task_type=\"VL\"  # Vision-Language task\n",
        "\n",
        ")\n",
        "formulamodel = get_peft_model(formulamodel,lora_config)\n",
        "torch.compile(formulamodel)\n",
        "formulamodel.merge_and_unload()\n",
        "formulamodel.load_state_dict(torch.load(\"/content/model.pth\"))\n",
        "\n",
        "textprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\n",
        "\n",
        "textmodel = VisionEncoderDecoderModel.from_pretrained('/root/.cache/kagglehub/datasets/doggasivakumar/smallocr/versions/1/checkpoint-6770').to(device)\n",
        "#model = VisionEncoderDecoderModel.from_pretrained('/kaggle/input/smallocr/checkpoint-6770').to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNYFwbdP-aje"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/boomb0om/CRAFT-text-detection/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0rkkl8rR-pt-",
        "outputId": "d8d41254-a0b4-41b4-c0ac-544000991d3a"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.10/dist-packages/huggingface_hub/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9e8b4f6bbf3d>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mCRAFT\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCRAFTModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_polygons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/CRAFT/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCRAFTModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_polygons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_area\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolygons_area\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/CRAFT/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhf_hub_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mCRAFT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcraft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCRAFT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_CRAFT_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.10/dist-packages/huggingface_hub/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor\n",
        "import torch\n",
        "from PIL import Image\n",
        "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from CRAFT import CRAFTModel, draw_polygons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSPAlm2oTHtX",
        "outputId": "6318e536-1177-4b43-e4e7-0924a80de43d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setp 2\n",
            "setp 3\n",
            "Recognized Text Paragraphs:\n",
            "\n",
            "\\documentclass{article}\n",
            "\\usepackage{amsmath}\n",
            "\n",
            "\\begin{document}\n",
            "\n",
            "\\title{Digital Document}\n",
            "\\author{IE 643}\n",
            "\\date{\\today}\n",
            "\\maketitle\n",
            "\n",
            "\\begin{verbatim}\n",
            "['STATESHIP']\n",
            " \n",
            "    \\end{verbatim}\n",
            "    \\end{document}\n",
            "LaTeX compilation success:\n"
          ]
        }
      ],
      "source": [
        "\n",
        "cropped_images = []\n",
        "special_character_images = []\n",
        "recognized_texts = []\n",
        "\n",
        "def recognize_text_from_images(cropped_images):\n",
        "    \"\"\"\n",
        "    Recognize text from a list of cropped images.\n",
        "    :param cropped_images: List of cropped images.\n",
        "    :return: List of recognized text strings.\n",
        "    \"\"\"\n",
        "\n",
        "    for cropped_image in cropped_images:\n",
        "        # Convert OpenCV BGR image to PIL format for processing\n",
        "        image = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # Process the image and move pixel values to the correct device\n",
        "        pixel_values = textprocessor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
        "\n",
        "        # Generate text from the model\n",
        "        generated_ids = textmodel.generate(pixel_values)\n",
        "        generated_text = textprocessor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "        if generated_text == \"SPECIAL_CHARACTER\":\n",
        "            special_character_images.append(cropped_image)\n",
        "            pixel_values = formulaprocessor(images=image, return_tensors=\"pt\").pixel_values\n",
        "            pixel_values = pixel_values.to(device)\n",
        "\n",
        "            result_tensor = formulamodel.generate(\n",
        "                        pixel_values,\n",
        "                        max_length=max_length,\n",
        "                        bad_words_ids=[[formulaprocessor.tokenizer.unk_token_id]]\n",
        "                        ) # generate id tensor\n",
        "\n",
        "            generated_text = formulaprocessor.batch_decode(result_tensor, skip_special_tokens=True) # Using the processor to decode the result\n",
        "            generated_text = formulaprocessor.post_process_generation(generated_text, fix_markdown=False)\n",
        "\n",
        "    recognized_texts.append(generated_text)\n",
        "\n",
        "    return recognized_texts\n",
        "\n",
        "def upload_pdf(pdf_path):\n",
        "    _, ext = os.path.splitext(pdf_path)\n",
        "    ext = ext.lower()\n",
        "    if ext == \".pdf\":\n",
        "      images = convert_from_path(pdf_path, dpi=200, fmt=\"jpeg\")\n",
        "    elif ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\"]:\n",
        "      images = [cv2.imread(pdf_path)]\n",
        "    print(\"setp 2\")\n",
        "    # Save images to the current working directory and collect their paths\n",
        "    image_paths = []\n",
        "    merged_paragraphs = []\n",
        "     #  initialize empty list\n",
        "    full_text=\"\"\n",
        "    for i, image in enumerate(images):\n",
        "\n",
        "        print(\"setp 3\")\n",
        "        if ext == \".pdf\":\n",
        "            filename = os.path.join('/content/working/converted', f'converted_image_{i + 1}.jpeg')\n",
        "            image.save(filename, \"JPEG\")\n",
        "            img = cv2.imread(filename)\n",
        "            image=img\n",
        "\n",
        "             #  Creating a copy of the image to make changes\n",
        "            grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  #  Converting to greyscale\n",
        "            # Apply Otsu filtering\n",
        "            otsu1 = cv2.threshold(grey, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)[1]\n",
        "#             horz_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (14, 1))\n",
        "#             # The above line creates a rectangular kernel of dimension (14, 1)\n",
        "#             lines = cv2.morphologyEx(otsu1, cv2.MORPH_OPEN, horz_kernel, iterations=2)\n",
        "\n",
        "#             cnts = cv2.findContours(lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            # findContours returns the contours and the hierarchy for opencv version 4.0 and later\n",
        "            # earlier versions return: [image, contours, hierarchy]\n",
        "#             cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
        "#             for c in cnts:\n",
        "#                 cv2.drawContours(img, [c], -1, (255, 255, 255), 2)\n",
        "            grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            otsu2 = cv2.threshold(grey, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)[1]\n",
        "            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (14,9))\n",
        "            dilation = cv2.dilate(otsu2, kernel, iterations = 1)\n",
        "            contours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL,\n",
        "                                        cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "            count = 0  #  keep a count for the words (optional)\n",
        "\n",
        "\n",
        "            y_positions=[]\n",
        "            for c in contours:\n",
        "                x, y, w, h = cv2.boundingRect(c)  #  get coordinates of contouts\n",
        "                if w>50 and h>25:  #  threshold for size of bounding box\n",
        "                    count += 1\n",
        "                    cropped = image[y:y+h, x:x+w]  # Crop the image\n",
        "\n",
        "                    # Calculate scale to fit the larger side into 224 while keeping aspect ratio\n",
        "                    scale = min(224 / w, 224 / h)\n",
        "                    new_w, new_h = int(w * scale), int(h * scale)\n",
        "\n",
        "                    # Resize the cropped image\n",
        "                    resized = cv2.resize(cropped, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "                    # Create a 224x224 white background\n",
        "                    padded = np.ones((224, 224, 3), dtype=np.uint8) * 255 if len(resized.shape) == 3 else np.ones((224, 224), dtype=np.uint8) * 255\n",
        "\n",
        "                    # Calculate padding to center the image\n",
        "                    x_offset = (224 - new_w) // 2\n",
        "                    y_offset = (224 - new_h) // 2\n",
        "\n",
        "                    # Place the resized image on the white background\n",
        "                    padded[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized\n",
        "\n",
        "                    # Append to the list\n",
        "                    cropped_images.append(padded)\n",
        "            #         cropped_images.append(cv2.resize(otsu2[y:h+y, x:w+x], (224, 224))) # Resizing to fixed size\n",
        "#                     rect = cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2) # Draw Bounding boxes\n",
        "                    y_positions.append(np.mean([y,y+h]))\n",
        "\n",
        "            print(count, \" words extracted!\")     # Recognize text from cropped images\n",
        "            recognized_texts = recognize_text_from_images(cropped_images)\n",
        "            print(len(recognized_texts))\n",
        "    #         recognized_texts = [\"sorry will be soon available\",\"facing issues with resources\"]\n",
        "            # Combine recognized texts into paragraphs based on y-positions\n",
        "\n",
        "            current_paragraph = \"\"\n",
        "            threshold = 70  # Define a threshold for merging based on y-coordinate proximity\n",
        "\n",
        "            for i, text in enumerate(recognized_texts):\n",
        "                if text.strip():  # If the recognized text is not empty\n",
        "                    if current_paragraph:  # If there is already some text in the paragraph\n",
        "                        # Check if current and last y-position are within the threshold\n",
        "                        if abs(y_positions[i] - y_positions[i - 1]) < threshold:\n",
        "                            current_paragraph=  text.strip()+ \" \" + current_paragraph\n",
        "                        else:\n",
        "                            merged_paragraphs.append(current_paragraph.strip())\n",
        "                            current_paragraph = text.strip()+ \" \"   # Start a new paragraph\n",
        "                    else:\n",
        "                        current_paragraph = text.strip()+ \" \"   # Start a new paragraph\n",
        "\n",
        "            # Append any remaining text\n",
        "            if current_paragraph:\n",
        "                merged_paragraphs.append(current_paragraph.strip())\n",
        "        else:\n",
        "            cropped_img=image\n",
        "            orig_h, orig_w = cropped_img.shape[:2]\n",
        "\n",
        "            # Scale the image to fit within 224x224 while keeping the aspect ratio\n",
        "            scale = min(224 / orig_w, 224 / orig_h)\n",
        "            new_w, new_h = int(orig_w * scale), int(orig_h * scale)\n",
        "\n",
        "            # Resize the cropped image\n",
        "            resized = cv2.resize(cropped_img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            # Create a 224x224 white background with 3 channels if resized is in color\n",
        "            padded = np.ones((224, 224, 3), dtype=np.uint8) * 255 if len(resized.shape) == 3 else np.ones((224, 224), dtype=np.uint8) * 255\n",
        "\n",
        "            # Calculate padding to center the resized image\n",
        "            x_offset = (224 - new_w) // 2\n",
        "            y_offset = (224 - new_h) // 2\n",
        "\n",
        "            # Place the resized image on the white background\n",
        "            padded[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized\n",
        "\n",
        "            # Append to the list of cropped images\n",
        "            cropped_images.append(padded)\n",
        "\n",
        "            # Recognize text from the padded image and store in merged_paragraphs\n",
        "            merged_paragraphs.append(recognize_text_from_images([padded]))\n",
        "\n",
        "        # Print the recognized text paragraphs\n",
        "        print(\"Recognized Text Paragraphs:\")\n",
        "        for i, paragraph in enumerate(merged_paragraphs):\n",
        "            full_text+=f\"\"\"{paragraph}\n",
        "\"\"\"\n",
        "        full_text+= r\"\"\" \"\"\"\n",
        "\n",
        "    # Minimal LaTeX code for testing\n",
        "        # Minimal LaTeX code for testing\n",
        "    header = latex_code = r\"\"\"\n",
        "\\documentclass{article}\n",
        "\\usepackage{amsmath}\n",
        "\n",
        "\\begin{document}\n",
        "\n",
        "\\title{Digital Document}\n",
        "\\author{IE 643}\n",
        "\\date{\\today}\n",
        "\\maketitle\n",
        "\n",
        "\\begin{verbatim}\n",
        "\"\"\"\n",
        "    footer=r\"\"\"\n",
        "    \\end{verbatim}\n",
        "    \\end{document}\"\"\"\n",
        "\n",
        "    latex_code = header+full_text+footer\n",
        "    print(latex_code)\n",
        "    # Write LaTeX code to file in the current working directory\n",
        "    latex_file_path = os.path.join(os.getcwd(), \"output.tex\")\n",
        "    with open(latex_file_path, \"w\") as f:\n",
        "        f.write(latex_code)\n",
        "\n",
        "    # Compile the LaTeX file\n",
        "    process = subprocess.run(['pdflatex', latex_file_path], capture_output=True, text=True)\n",
        "\n",
        "    if process.returncode != 0:\n",
        "        print(\"LaTeX compilation failed:\")\n",
        "        print(process.stderr)\n",
        "\n",
        "    else:\n",
        "        print(\"LaTeX compilation success:\")\n",
        "        pdf_output_path = os.path.join(os.getcwd(), \"output.pdf\")\n",
        "\n",
        "upload_pdf(\"/root/.cache/kagglehub/datasets/doggasivakumar/dataset-model/versions/1/20241107_185154.jpg\")\n",
        "# upload_pdf(\"/content/WhatsApp Image 2024-11-26 at 14.44.18_3b9423c2.jpg\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "rIsqWZ779r86",
        "outputId": "cb3b05dd-7143-448f-885a-bdd42cc71441"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/root/.cache/kagglehub/datasets/doggasivakumar/dataset-model/versions/1/mathematical_image.jpg'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(\"/root/.cache/kagglehub/datasets/doggasivakumar/dataset-model/versions/1/mathematical_image.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DcEhNAGA1LOo",
        "outputId": "a55505de-2585-4b42-c415-3a8c1acda4e7"
      },
      "outputs": [
        {
          "data": {
            "text/latex": "$\\displaystyle \\\\tilde{L}(y,\\\\tilde{y})=-\\\\sum_{i=1}^{c}y_{i}\\\\cdot log\\\\tilde{y}_{i}\\n\\n $",
            "text/plain": [
              "<IPython.core.display.Math object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\\\n",
        "# from IPython.display import display, Math\n",
        "\n",
        "# # Assuming latex_code contains the LaTeX code generated in your previous code\n",
        "# # Replace this with the actual variable name containing your LaTeX code\n",
        "\n",
        "# # Example:  Let's say your generated latex code is stored in a variable called \"latex_code\"\n",
        "# #  from the previous code block.\n",
        "# # In this case the below code is just a example and needs to be uncommented to see\n",
        "# # the formula.\n",
        "\n",
        "# # display(Math(latex_code))\n",
        "\n",
        "\n",
        "# # Example usage:\n",
        "# latex_code_example = r\"\\\\tilde{L}(y,\\\\tilde{y})=-\\\\sum_{i=1}^{c}y_{i}\\\\cdot log\\\\tilde{y}_{i}\\n\\n \" # Replace with the correct variable\n",
        "# display(Math(latex_code_example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "j1vN6m05vMv4",
        "outputId": "a4119ed8-9793-4879-fb0a-079cde313cd1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-99043288-652e-4953-b61c-87f6fd997153\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-99043288-652e-4953-b61c-87f6fd997153\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving prem.jpg to prem.jpg\n",
            "User uploaded file \"prem.jpg\" with length 16340 bytes\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "  name=fn, length=len(uploaded[fn])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtMN8QU50y5R",
        "outputId": "9e003e63-1e1e-4db6-b498-bba2832001a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\\\sum_{k=2}^{\\\\infty}\\\\frac{\\\\sum_{j=8}^{\\\\infty}\\\\frac{\\\\sum_{j=8}^{\\\\infty}}{\\\\sum_{k=2}^{\\\\infty}\\\\frac{\\\\sum_{j}}{\\\\sum_{j=8}}}\\n\\n ']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVc5tS_AXfAR",
        "outputId": "e85d151c-5963-4dc7-b204-5c51fabd8923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred during PDF generation:\n",
            "This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) (preloaded format=pdflatex)\n",
            " restricted \\write18 enabled.\n",
            "entering extended mode\n",
            "(./document.tex\n",
            "LaTeX2e <2021-11-15> patch level 1\n",
            "L3 programming layer <2022-01-21>\n",
            "(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls\n",
            "Document Class: article 2021/10/04 v1.4n Standard LaTeX document class\n",
            "(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo))\n",
            "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsmath.sty\n",
            "For additional information on amsmath, use the `?' option.\n",
            "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amstext.sty\n",
            "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsgen.sty))\n",
            "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsbsy.sty)\n",
            "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsopn.sty))\n",
            "(/usr/share/texlive/texmf-dist/tex/latex/l3backend/l3backend-pdftex.def)\n",
            "No file document.aux.\n",
            "\n",
            "! LaTeX Error: There's no line here to end.\n",
            "\n",
            "See the LaTeX manual or LaTeX Companion for explanation.\n",
            "Type  H <return>  for immediate help.\n",
            " ...                                              \n",
            "                                                  \n",
            "l.13 \\\\t\n",
            "        ilde{L}(y,\\\\tilde{y})=-\\\\sum_{i=1}^{c}y_{i}\\\\cdot log\\\\tilde{y}_{i}\\n\\n\n",
            "\n",
            "! Missing $ inserted.\n",
            "<inserted text> \n",
            "                $\n",
            "l.13 \\\\tilde{L}(y,\\\\tilde{y})=-\\\\sum_\n",
            "                                     {i=1}^{c}y_{i}\\\\cdot log\\\\tilde{y}_{i}\\n\\n\n",
            "\n",
            "! Undefined control sequence.\n",
            "l.13 ...m_{i=1}^{c}y_{i}\\\\cdot log\\\\tilde{y}_{i}\\n\n",
            "                                                  \\n\n",
            "! Undefined control sequence.\n",
            "l.13 ...{i=1}^{c}y_{i}\\\\cdot log\\\\tilde{y}_{i}\\n\\n\n",
            "                                                  \n",
            "! Missing $ inserted.\n",
            "<inserted text> \n",
            "                $\n",
            "l.14 \n",
            "     \n",
            "[1{/var/lib/texmf/fonts/map/pdftex/updmap/pdftex.map}] (./document.aux) )\n",
            "(see the transcript file for additional information)</usr/share/texlive/texmf-d\n",
            "ist/fonts/type1/public/amsfonts/cm/cmmi10.pfb></usr/share/texlive/texmf-dist/fo\n",
            "nts/type1/public/amsfonts/cm/cmmi7.pfb></usr/share/texlive/texmf-dist/fonts/typ\n",
            "e1/public/amsfonts/cm/cmr10.pfb></usr/share/texlive/texmf-dist/fonts/type1/publ\n",
            "ic/amsfonts/cm/cmr12.pfb></usr/share/texlive/texmf-dist/fonts/type1/public/amsf\n",
            "onts/cm/cmr17.pfb></usr/share/texlive/texmf-dist/fonts/type1/public/amsfonts/cm\n",
            "/cmr7.pfb>\n",
            "Output written on document.pdf (1 page, 60373 bytes).\n",
            "Transcript written on document.log.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Define the LaTeX code\n",
        "latex_code = r\"\"\"\n",
        "\\documentclass{article}\n",
        "\\usepackage{amsmath}\n",
        "\n",
        "\\begin{document}\n",
        "\n",
        "\\title{Digital Document}\n",
        "\\author{IE 643}\n",
        "\\date{\\today}\n",
        "\\maketitle\n",
        "\n",
        "\n",
        "\\\\tilde{L}(y,\\\\tilde{y})=-\\\\sum_{i=1}^{c}y_{i}\\\\cdot log\\\\tilde{y}_{i}\\n\\n\n",
        "\n",
        "\\end{document}\n",
        "\"\"\"\n",
        "\n",
        "# Specify the output file names\n",
        "latex_file = \"document.tex\"\n",
        "output_pdf = \"document.pdf\"\n",
        "\n",
        "# Write the LaTeX code to a .tex file\n",
        "with open(latex_file, \"w\") as file:\n",
        "    file.write(latex_code)\n",
        "\n",
        "# Compile the LaTeX file to a PDF\n",
        "try:\n",
        "    process = subprocess.run(\n",
        "        [\"pdflatex\", \"-interaction=nonstopmode\", latex_file],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        check=True\n",
        "    )\n",
        "    print(\"PDF generated successfully.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(\"An error occurred during PDF generation:\")\n",
        "    print(e.stdout)\n",
        "    print(e.stderr)\n",
        "\n",
        "# Clean up auxiliary files generated by pdflatex\n",
        "for ext in [\".aux\", \".log\"]:\n",
        "    aux_file = latex_file.replace(\".tex\", ext)\n",
        "    if os.path.exists(aux_file):\n",
        "        os.remove(aux_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "vJARE7P3VuKg",
        "outputId": "132b1e4a-8232-469f-87d4-c23cbbc81338"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   1198 MiB |   1198 MiB |      0 B   |      0 B   |\\n|       from large pool |   1077 MiB |   1077 MiB |      0 B   |      0 B   |\\n|       from small pool |    120 MiB |    120 MiB |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   1198 MiB |   1198 MiB |      0 B   |      0 B   |\\n|       from large pool |   1077 MiB |   1077 MiB |      0 B   |      0 B   |\\n|       from small pool |    120 MiB |    120 MiB |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   1194 MiB |   1194 MiB |      0 B   |      0 B   |\\n|       from large pool |   1073 MiB |   1073 MiB |      0 B   |      0 B   |\\n|       from small pool |    120 MiB |    120 MiB |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   1586 MiB |   1586 MiB |      0 B   |      0 B   |\\n|       from large pool |   1412 MiB |   1412 MiB |      0 B   |      0 B   |\\n|       from small pool |    174 MiB |    174 MiB |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 397248 KiB | 397248 KiB |      0 B   |      0 B   |\\n|       from large pool | 342160 KiB | 342160 KiB |      0 B   |      0 B   |\\n|       from small pool |  55088 KiB |  55088 KiB |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     849    |     849    |       0    |       0    |\\n|       from large pool |     114    |     114    |       0    |       0    |\\n|       from small pool |     735    |     735    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     849    |     849    |       0    |       0    |\\n|       from large pool |     114    |     114    |       0    |       0    |\\n|       from small pool |     735    |     735    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     126    |     126    |       0    |       0    |\\n|       from large pool |      39    |      39    |       0    |       0    |\\n|       from small pool |      87    |      87    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |     104    |     104    |       0    |       0    |\\n|       from large pool |      19    |      19    |       0    |       0    |\\n|       from small pool |      85    |      85    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "# Clear the GPU memory cache\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "# Run garbage collection\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "# Optional: Force release of unused CUDA memory\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "  with torch.cuda.device(0):\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    torch.cuda.ipc_collect()\n",
        "\n",
        "\n",
        "\n",
        "# Print a memory summary (optional)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "  torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "# Clear all cache on the GPU\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "# You can also reset all memory allocations (optional):\n",
        "\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "torch.cuda.reset_accumulated_memory_stats()\n",
        "\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbjkgBCKWZ26"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51440bc411cb480f8bb2eca3239e1be1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "6abdf6f7d6004786a230528066745e27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3a58a0280414e08a77dba63d43dc8aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc42b33d2f774b1a8c9b160ce64b9d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef3794f8875f4a7cb8b87650aa73ecdc"
            ],
            "layout": "IPY_MODEL_51440bc411cb480f8bb2eca3239e1be1"
          }
        },
        "ef3794f8875f4a7cb8b87650aa73ecdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6abdf6f7d6004786a230528066745e27",
            "placeholder": "​",
            "style": "IPY_MODEL_c3a58a0280414e08a77dba63d43dc8aa",
            "value": "Kaggle credentials successfully validated."
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}