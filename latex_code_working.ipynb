{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 9742583,
          "sourceType": "datasetVersion",
          "datasetId": 5963696
        },
        {
          "sourceId": 9774111,
          "sourceType": "datasetVersion",
          "datasetId": 5987217
        },
        {
          "sourceId": 9786349,
          "sourceType": "datasetVersion",
          "datasetId": 5996133
        },
        {
          "sourceId": 9788004,
          "sourceType": "datasetVersion",
          "datasetId": 5997327
        },
        {
          "sourceId": 9788069,
          "sourceType": "datasetVersion",
          "datasetId": 5997374
        },
        {
          "sourceId": 9794799,
          "sourceType": "datasetVersion",
          "datasetId": 6002372
        },
        {
          "sourceId": 9851029,
          "sourceType": "datasetVersion",
          "datasetId": 6044645
        },
        {
          "sourceId": 9851960,
          "sourceType": "datasetVersion",
          "datasetId": 6045329
        },
        {
          "sourceId": 9852035,
          "sourceType": "datasetVersion",
          "datasetId": 6045372
        },
        {
          "sourceId": 9852088,
          "sourceType": "datasetVersion",
          "datasetId": 6045399
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "latex_code_working",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sivadogga007/Genny_IE643_Course_Project/blob/master/latex_code_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()",
"# please mail 24m1528@iitb.ac.in for credentials. I will reply asap"
      ],
      "metadata": {
        "id": "N5zliarZFyL9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "doggasivakumar_gurgurov_model_im2latest_path = kagglehub.dataset_download('doggasivakumar/gurgurov-model-im2latest')\n",
        "doggasivakumar_nadikcew_path = kagglehub.dataset_download('doggasivakumar/nadikcew')\n",
        "doggasivakumar_smallocr_path = kagglehub.dataset_download('doggasivakumar/smallocr')\n",
        "doggasivakumar_modell_path = kagglehub.dataset_download('doggasivakumar/modell')\n",
        "doggasivakumar_pribe3ec_path = kagglehub.dataset_download('doggasivakumar/pribe3ec')\n",
        "doggasivakumar_loradapter_path = kagglehub.dataset_download('doggasivakumar/loradapter')\n",
        "doggasivakumar_0911lorafinetune_path = kagglehub.dataset_download('doggasivakumar/0911lorafinetune')\n",
        "doggasivakumar_image_test_path = kagglehub.dataset_download('doggasivakumar/image-test')\n",
        "doggasivakumar_dataset_model_path = kagglehub.dataset_download('doggasivakumar/dataset-model')\n",
        "doggasivakumar_mathdocument_path = kagglehub.dataset_download('doggasivakumar/mathdocument')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "JKKxCQidFyMB"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/boomb0om/CRAFT-text-detection/"
      ],
      "metadata": {
        "trusted": true,
        "id": "asACd24jFyMD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "trusted": true,
        "id": "mpjbMKLxFyME"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "# from CRAFT import CRAFTModel, draw_polygons\n",
        "\n",
        "# model = CRAFTModel('weights/', 'cuda', use_refiner=True, fp16=True)\n",
        "# img = Image.open('/kaggle/working/converted_image_1.jpeg').convert('RGB')\n",
        "# polygons = model.get_polygons(img)"
      ],
      "metadata": {
        "trusted": true,
        "id": "QO6dEZ1RFyME"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# draw_polygons(img, polygons)"
      ],
      "metadata": {
        "trusted": true,
        "id": "9jYOMCSVFyMF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install jiwer\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install -U accelerate\n",
        "!pip install matplotlib\n",
        "!pip install tensorboard"
      ],
      "metadata": {
        "trusted": true,
        "id": "zbV3lJoxFyMG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor\n",
        "import torch\n",
        "from PIL import Image\n",
        "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from CRAFT import CRAFTModel, draw_polygons\n",
        "from fastapi import FastAPI, File, UploadFile,BackgroundTasks\n",
        "from fastapi.responses import JSONResponse, FileResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import uvicorn\n",
        "from pdf2image import convert_from_path\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.responses import JSONResponse\n",
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from transformers import NougatProcessor, VisionEncoderDecoderModel\n",
        "import numpy as np\n",
        "max_length = 100 # defing max length of output\n",
        "\n",
        "import torch\n",
        "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-26T11:10:31.143093Z",
          "iopub.execute_input": "2024-11-26T11:10:31.143498Z",
          "iopub.status.idle": "2024-11-26T11:10:35.674345Z",
          "shell.execute_reply.started": "2024-11-26T11:10:31.14346Z",
          "shell.execute_reply": "2024-11-26T11:10:35.673349Z"
        },
        "trusted": true,
        "id": "S_fk8RWTFyMH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# device = xm.xla_device()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-26T11:10:45.166476Z",
          "iopub.execute_input": "2024-11-26T11:10:45.167529Z",
          "iopub.status.idle": "2024-11-26T11:10:45.206148Z",
          "shell.execute_reply.started": "2024-11-26T11:10:45.167487Z",
          "shell.execute_reply": "2024-11-26T11:10:45.204932Z"
        },
        "trusted": true,
        "id": "YPbzKT8TFyMH",
        "outputId": "ac4ecde3-57be-427a-8226-664c593ac9de"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "cuda\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Poppler\n",
        "\n",
        "!apt-get install -y poppler-utils\n",
        "\n",
        "# Install LaTeX\n",
        "!apt-get install -y texlive texlive-latex-extra texlive-fonts-recommended\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "7mPXqm2CFyMJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = VisionEncoderDecoderModel.from_pretrained('/kaggle/input/modell/archive/kaggle/working/trocr_handwritten/checkpoint-13540').to(device)\n",
        "#model = VisionEncoderDecoderModel.from_pretrained('/kaggle/input/smallocr/checkpoint-6770').to(device)\n",
        "tokenizer = NougatProcessor.from_pretrained(r\"CuiSiwei/nougat-for-formula\", max_length = max_length) # Replace with your path\n",
        "feature_extractor = NougatProcessor.from_pretrained(r\"CuiSiwei/nougat-for-formula\", max_length = max_length) # Replace with your path\n",
        "\n",
        "mmodel = VisionEncoderDecoderModel.from_pretrained(r\"CuiSiwei/nougat-for-formula\").to(device) # Replace with your path\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "\n",
        "    r=16,\n",
        "\n",
        "    lora_alpha=16,\n",
        "\n",
        "    # target_modules=[\n",
        "\n",
        "    #     # Encoder (Swin Transformer) modules\n",
        "\n",
        "    #     \"attn.qkv\",\n",
        "\n",
        "    #     \"attn.proj\",\n",
        "\n",
        "    #     \"mlp.fc1\",\n",
        "\n",
        "    #     \"mlp.fc2\",\n",
        "\n",
        "    #     # Decoder (GPT-2) modules\n",
        "\n",
        "    #     \"c_attn\",\n",
        "\n",
        "    #     \"c_proj\",\n",
        "\n",
        "    #     \"c_fc\",\n",
        "\n",
        "    #     \"attn.c_proj\",\n",
        "\n",
        "    # ],\n",
        "\n",
        "    target_modules=\"all-linear\",\n",
        "\n",
        "    lora_dropout=0.1,\n",
        "\n",
        "    bias=\"none\"\n",
        "\n",
        "    # task_type=\"VL\"  # Vision-Language task\n",
        "\n",
        ")\n",
        "mmodel = get_peft_model(mmodel,lora_config)\n",
        "torch.compile(mmodel)\n",
        "mmodel.merge_and_unload()\n",
        "mmodel.load_state_dict(torch.load(\"/kaggle/working/model.pth\"))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-26T11:10:48.272746Z",
          "iopub.execute_input": "2024-11-26T11:10:48.273162Z",
          "iopub.status.idle": "2024-11-26T11:11:04.881931Z",
          "shell.execute_reply.started": "2024-11-26T11:10:48.273124Z",
          "shell.execute_reply": "2024-11-26T11:11:04.880749Z"
        },
        "trusted": true,
        "id": "1P7qChmnFyMK",
        "outputId": "b2e82d7f-e7a0-483e-c6e1-64f41865faf3"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nVisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n/tmp/ipykernel_388/1158876135.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  mmodel.load_state_dict(torch.load(\"/kaggle/working/model.pth\"))\n",
          "output_type": "stream"
        },
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<All keys matched successfully>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # load model, tokenizer, and feature extractor\n",
        "# mmodel = VisionEncoderDecoderModel.from_pretrained(\"DGurgurov/im2latex\").to(device)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"DGurgurov/im2latex\")\n",
        "# feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-base-patch4-window7-224-in22k\") # using the original feature extractor for now\n",
        "# # tokenizer.pad_token = tokenizer.pad_token\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "QBwz6PstFyML"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-Levenshtein"
      ],
      "metadata": {
        "trusted": true,
        "id": "OW1qJXzcFyML"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# generate LaTeX formula\n",
        "# prepare an image\n",
        "image = cv2.imread(\"/kaggle/input/dataset-model/mathematical_image.jpg\")\n",
        "orig_h, orig_w = image.shape[:2]\n",
        "\n",
        "# Scale the image to fit within 224x224 while keeping the aspect ratio\n",
        "scale = min(224 / orig_w, 224 / orig_h)\n",
        "new_w, new_h = int(orig_w * scale), int(orig_h * scale)\n",
        "\n",
        "# Resize the cropped image\n",
        "resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "# Create a 224x224 white background with 3 channels if resized is in color\n",
        "padded = np.ones((224, 224, 3), dtype=np.uint8) * 255 if len(resized.shape) == 3 else np.ones((224, 224), dtype=np.uint8) * 255\n",
        "\n",
        "# Calculate padding to center the resized image\n",
        "x_offset = (224 - new_w) // 2\n",
        "y_offset = (224 - new_h) // 2\n",
        "\n",
        "# Place the resized image on the white background\n",
        "padded[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized\n",
        "pixel_values = feature_extractor(images=padded, return_tensors=\"pt\").pixel_values\n",
        "pixel_values = pixel_values.to(device)\n",
        "\n",
        "generated_ids = mmodel.generate(pixel_values)\n",
        "generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated LaTeX formula by model :\", generated_texts[0])\n",
        "# Load the PeftModel correctly and assign it to a variable\n",
        "loaded_model = PeftModel.from_pretrained(mmodel, \"/kaggle/input/0911lorafinetune/checkpoints/checkpoints/checkpoint_step_2000\")\n",
        "\n",
        "\n",
        "# Move pixel_values to the same device as the model\n",
        " # Assuming 'device' is defined as 'cuda:0'\n",
        "\n",
        "# generate LaTeX formula\n",
        "generated_ids = loaded_model.generate(pixel_values)\n",
        "generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated LaTeX formula by fine tuned model :\", generated_texts[0])\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Mc7YIa5VFyMM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device=\"cuda\""
      ],
      "metadata": {
        "trusted": true,
        "id": "NvxQX-h5FyMM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import NougatProcessor, VisionEncoderDecoderModel\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "max_length = 100 # defing max length of output\n",
        "processor = NougatProcessor.from_pretrained(r\"CuiSiwei/nougat-for-formula\", max_length = max_length) # Replace with your path\n",
        "# model = VisionEncoderDecoderModel.from_pretrained(r\"CuiSiwei/nougat-for-formula\").to(device) # Replace with your path\n",
        "\n",
        "# generate LaTeX formula\n",
        "# prepare an image\n",
        "image = cv2.imread(\"/kaggle/input/dataset-model/mathematical_image.jpg\")\n",
        "orig_h, orig_w = image.shape[:2]\n",
        "\n",
        "# Scale the image to fit within 224x224 while keeping the aspect ratio\n",
        "scale = min(224 / orig_w, 224 / orig_h)\n",
        "new_w, new_h = int(orig_w * scale), int(orig_h * scale)\n",
        "\n",
        "# Resize the cropped image\n",
        "resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "# Create a 224x224 white background with 3 channels if resized is in color\n",
        "padded = np.ones((224, 224, 3), dtype=np.uint8) * 255 if len(resized.shape) == 3 else np.ones((224, 224), dtype=np.uint8) * 255\n",
        "\n",
        "# Calculate padding to center the resized image\n",
        "x_offset = (224 - new_w) // 2\n",
        "y_offset = (224 - new_h) // 2\n",
        "\n",
        "# Place the resized image on the white background\n",
        "padded[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized\n",
        "pixel_values = processor(images=padded, return_tensors=\"pt\").pixel_values\n",
        "pixel_values = pixel_values.to(device)\n",
        "\n",
        "result_tensor = model.generate(\n",
        "            pixel_values,\n",
        "            max_length=max_length,\n",
        "            bad_words_ids=[[processor.tokenizer.unk_token_id]]\n",
        "              ) # generate id tensor\n",
        "\n",
        "result = processor.batch_decode(result_tensor, skip_special_tokens=True) # Using the processor to decode the result\n",
        "result = processor.post_process_generation(result, fix_markdown=False)\n",
        "\n",
        "print(*result)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "S2QbihWgFyMM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "AVHVQ3SiFyMM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "IYaPpQ67FyMN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "A2Ugl7ELFyMN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-Levenshtein"
      ],
      "metadata": {
        "trusted": true,
        "id": "Y97SJsEHFyMN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Step 1: Define the Kaggle credentials dictionary\n",
        "kaggle_credentials = {\"username\":\"sivadogga\",\"key\":\"b56da0155f5e3e607c8cffaf048d686c\"}\n",
        "\n",
        "# Step 2: Save the dictionary as kaggle.json\n",
        "kaggle_json_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(kaggle_json_path), exist_ok=True)\n",
        "\n",
        "# Write the credentials to the file\n",
        "with open(kaggle_json_path, \"w\") as f:\n",
        "    json.dump(kaggle_credentials, f)\n",
        "\n",
        "# Set the correct permissions for security\n",
        "os.chmod(kaggle_json_path, 0o600)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "1p5RuskDFyMN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "wD6eNAJcFyMN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Step 1: Define the Kaggle credentials dictionary\n",
        "kaggle_credentials = {\"username\":\"sivadogga\",\"key\":\"b56da0155f5e3e607c8cffaf048d686c\"}\n",
        "\n",
        "# Step 2: Save the dictionary as kaggle.json\n",
        "kaggle_json_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(kaggle_json_path), exist_ok=True)\n",
        "\n",
        "# Write the credentials to the file\n",
        "with open(kaggle_json_path, \"w\") as f:\n",
        "    json.dump(kaggle_credentials, f)\n",
        "\n",
        "# Set the correct permissions for security\n",
        "os.chmod(kaggle_json_path, 0o600)\n",
        "\n",
        "\n",
        "!kaggle kernels output sivadogga/sharingbe -p /kaggle/working/"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZsdcAaM6FyMO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Working code of latex using craft model for word segmentation and text model for extracting text and mathematical equation recognition model*"
      ],
      "metadata": {
        "id": "v1fJORmGFyMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "polygon_model = CRAFTModel('weights', device , use_refiner=True, fp16=True)\n",
        "PERMANENT_DIR = \"pdf_images\"\n",
        "Path(PERMANENT_DIR).mkdir(parents=True, exist_ok=True)  # Creates the directory if it doesn't exist\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-26T11:11:19.912395Z",
          "iopub.execute_input": "2024-11-26T11:11:19.913162Z",
          "iopub.status.idle": "2024-11-26T11:11:22.809769Z",
          "shell.execute_reply.started": "2024-11-26T11:11:19.913124Z",
          "shell.execute_reply": "2024-11-26T11:11:22.808707Z"
        },
        "trusted": true,
        "id": "jI_gxCtmFyMP",
        "outputId": "069d47dd-89b6-4ca5-a5c6-824bb11ee5c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'cached_download' (from 'huggingface_hub.file_download') is deprecated and will be removed from version '0.26'. Use `hf_hub_download` instead.\n  warnings.warn(warning_message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:672: FutureWarning: 'cached_download' is the legacy way to download files from the HF hub, please consider upgrading to 'hf_hub_download'\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/CRAFT/craft.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  net.load_state_dict(copyStateDict(torch.load(chekpoint_path, map_location=torch.device('cpu'))))\n/opt/conda/lib/python3.10/site-packages/CRAFT/refinenet.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  refine_net.load_state_dict(copyStateDict(torch.load(chekpoint_path, map_location=torch.device('cpu'))))\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "def load_image(image_path):\n",
        "    \"\"\"Load an image from a file.\"\"\"\n",
        "    return cv2.imread(image_path)\n",
        "\n",
        "\n",
        "def crop_polygons(image, polygons):\n",
        "    \"\"\"\n",
        "    Crop images based on the provided polygons.\n",
        "    :param image: The original image.\n",
        "    :param polygons: List of polygons defining the areas to crop.\n",
        "    :return: List of cropped images and their y-positions.\n",
        "    \"\"\"\n",
        "    # Convert PIL Image to NumPy array for OpenCV compatibility\n",
        "    image = np.array(image)\n",
        "\n",
        "\n",
        "    y_positions = []\n",
        "\n",
        "    for i, polygon in enumerate(polygons):\n",
        "        # Create a mask for the polygon\n",
        "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "        cv2.fillPoly(mask, [np.array(polygon, dtype=np.int32)], 255)\n",
        "\n",
        "        # Get the bounding box of the polygon\n",
        "        x, y, w, h = cv2.boundingRect(np.array(polygon, dtype=np.int32))\n",
        "\n",
        "        # Crop the image using the mask\n",
        "        masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
        "        cropped_image = masked_image[y:y+h, x:x+w]\n",
        "\n",
        "        # Resize the cropped image for the text recognition model\n",
        "\n",
        "\n",
        "        cropped_images.append(cropped_image)\n",
        "        y_positions.append(np.mean([point[1] for point in polygon]))  # Mean y-coordinate\n",
        "        filename = os.path.join('/kaggle/working/pdf_images', f'sytcropped_image_{i + 1}.png')\n",
        "        cv2.imwrite(filename, cropped_image)\n",
        "    return cropped_images, y_positions\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-26T11:11:32.010117Z",
          "iopub.execute_input": "2024-11-26T11:11:32.010632Z",
          "iopub.status.idle": "2024-11-26T11:11:32.019749Z",
          "shell.execute_reply.started": "2024-11-26T11:11:32.010591Z",
          "shell.execute_reply": "2024-11-26T11:11:32.018512Z"
        },
        "trusted": true,
        "id": "OdiRx0XZFyMQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=100\n",
        "def recognize_text_from_images(cropped_images):\n",
        "    \"\"\"\n",
        "    Recognize text from a list of cropped images.\n",
        "    :param cropped_images: List of cropped images.\n",
        "    :return: List of recognized text strings.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    for cropped_image in cropped_images:\n",
        "        # Convert OpenCV BGR image to PIL format for processing\n",
        "        image = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
        "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
        "        generated_ids = model.generate(pixel_values)\n",
        "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        # Process the image and move pixel values to the correct device\n",
        "        pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
        "\n",
        "\n",
        "        # if generated_text == \"SPECIAL_CHARACTER\":\n",
        "        special_character_images.append(cropped_image)\n",
        "        generated_ids = mmodel.generate(pixel_values,max_length=max_length,\n",
        "                    bad_words_ids=[[processor.tokenizer.unk_token_id]])\n",
        "\n",
        "        generated_tex = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] # Using the processor to decode the result\n",
        "        generated_text += tokenizer.post_process_generation(generated_tex, fix_markdown=False)\n",
        "\n",
        "\n",
        "        recognized_texts.append(generated_text)\n",
        "\n",
        "\n",
        "    return recognized_texts"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-26T11:25:41.634665Z",
          "iopub.execute_input": "2024-11-26T11:25:41.635606Z",
          "iopub.status.idle": "2024-11-26T11:25:41.643044Z",
          "shell.execute_reply.started": "2024-11-26T11:25:41.635537Z",
          "shell.execute_reply": "2024-11-26T11:25:41.642119Z"
        },
        "trusted": true,
        "id": "iOs1ZjPcFyMR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**code using carft model for text detection**"
      ],
      "metadata": {
        "id": "UjGBtEOWFyMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cropped_images = []\n",
        "special_character_images = []\n",
        "recognized_texts = []\n",
        "\n",
        "\n",
        "def upload_pdf(pdf_path):\n",
        "    _, ext = os.path.splitext(pdf_path)\n",
        "    ext = ext.lower()\n",
        "    if ext == \".pdf\":\n",
        "      images = convert_from_path(pdf_path, dpi=200, fmt=\"jpeg\")\n",
        "    elif ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\"]:\n",
        "      images = [Image.open(pdf_path)]\n",
        "    print(\"setp 2\")\n",
        "    # Save images to the current working directory and collect their paths\n",
        "    image_paths = []\n",
        "    for i, image in enumerate(images):\n",
        "        print(\"hey\")\n",
        "\n",
        "        print(\"setp 3\")\n",
        "        if ext == \".pdf\":\n",
        "            filename = os.path.join('/kaggle/working/', f'converted_image_{i + 1}.jpeg')\n",
        "            image.save(filename, \"JPEG\")\n",
        "        image = image.convert('RGB')\n",
        "        polygons = polygon_model.get_polygons(image)\n",
        "        result=draw_polygons(image, polygons)\n",
        "        image_path = os.path.join(f\"/kaggle/working/page_{i + 1}.jpeg\")\n",
        "        result.save(image_path, \"JPEG\")\n",
        "        image_paths.append(image_path)\n",
        "            # Crop images and retrieve their y-positions\n",
        "        cropped_images, y_positions = crop_polygons(image, polygons)\n",
        "        print(len(cropped_images))\n",
        "        # Recognize text from cropped images\n",
        "        recognized_texts = recognize_text_from_images(cropped_images)\n",
        "        print(len(recognized_texts))\n",
        "#         recognized_texts = [\"sorry will be soon available\",\"facing issues with resources\"]\n",
        "        # Combine recognized texts into paragraphs based on y-positions\n",
        "        merged_paragraphs = []\n",
        "        current_paragraph = \"\"\n",
        "        threshold = 70  # Define a threshold for merging based on y-coordinate proximity\n",
        "\n",
        "        for i, text in enumerate(recognized_texts):\n",
        "            if str(text).strip():  # If the recognized text is not empty\n",
        "                if current_paragraph:  # If there is already some text in the paragraph\n",
        "                    # Check if current and last y-position are within the threshold\n",
        "                    if abs(y_positions[i] - y_positions[i - 1]) < threshold:\n",
        "                        current_paragraph=  str(text).strip()+ \" \" + current_paragraph\n",
        "                    else:\n",
        "                        merged_paragraphs.append(current_paragraph.strip())\n",
        "                        current_paragraph = str(text).strip()+ \"\\\\newline\"   # Start a new paragraph\n",
        "                else:\n",
        "                    current_paragraph = str(text).strip()+ \"\\\\newline\"   # Start a new paragraph\n",
        "\n",
        "        # Append any remaining text\n",
        "        if current_paragraph:\n",
        "            merged_paragraphs.append(current_paragraph.strip())\n",
        "\n",
        "        # Print the recognized text paragraphs\n",
        "        print(\"Recognized Text Paragraphs:\")\n",
        "        for i, paragraph in enumerate(merged_paragraphs):\n",
        "            text+=f\"\"\"{paragraph}\n",
        "\"\"\"\n",
        "        text+= r\"\"\"\\\\newpage\"\"\"\n",
        "\n",
        "    # Minimal LaTeX code for testing\n",
        "        # Minimal LaTeX code for testing\n",
        "    header = latex_code = r\"\"\"\n",
        "\\documentclass{article}\n",
        "\\usepackage{amsmath}\n",
        "\n",
        "\\begin{document}\n",
        "\n",
        "\\title{Digital Document}\n",
        "\\author{IE 643}\n",
        "\\date{\\today}\n",
        "\\maketitle\n",
        "\n",
        "\\begin{verbatim}\n",
        "\"\"\"\n",
        "    footer=r\"\"\"\n",
        "    \\end{verbatim}\n",
        "    \\end{document}\"\"\"\n",
        "\n",
        "    latex_code = header+str(text)+footer\n",
        "    print(latex_code)\n",
        "    # Write LaTeX code to file in the current working directory\n",
        "    latex_file_path = os.path.join(os.getcwd(), \"output.tex\")\n",
        "    with open(latex_file_path, \"w\") as f:\n",
        "        f.write(latex_code)\n",
        "\n",
        "    # Compile the LaTeX file\n",
        "    process = subprocess.run(['pdflatex', latex_file_path], capture_output=True, text=True)\n",
        "\n",
        "    if process.returncode != 0:\n",
        "        print(\"LaTeX compilation failed:\")\n",
        "        print(process.stderr)\n",
        "\n",
        "    else:\n",
        "        print(\"LaTeX compilation success:\")\n",
        "        pdf_output_path = os.path.join(os.getcwd(), \"ie643_output.pdf\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cleanup_files(file_paths):\n",
        "    \"\"\"Delete files from provided paths.\"\"\"\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            if os.path.exists(file_path):\n",
        "                os.remove(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to delete {file_path}: {e}\")\n",
        "\n",
        "\n",
        "upload_pdf(\"/kaggle/input/mathdocument/handwritten_mathematical.pdf\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-26T11:25:44.704384Z",
          "iopub.execute_input": "2024-11-26T11:25:44.704944Z",
          "iopub.status.idle": "2024-11-26T11:25:48.708402Z",
          "shell.execute_reply.started": "2024-11-26T11:25:44.704903Z",
          "shell.execute_reply": "2024-11-26T11:25:48.707204Z"
        },
        "trusted": true,
        "id": "y964qpATFyMT",
        "outputId": "c6f64728-fad4-4c1d-b8db-7ee64dfc31ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "setp 2\nhey\nsetp 3\n8\n8\nRecognized Text Paragraphs:\n\n\\documentclass{article}\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\title{Digital Document}\n\\author{IE 643}\n\\date{\\today}\n\\maketitle\n\n\\begin{verbatim}\ni-1\\tilde{t}_{1}\n\nThe formula for\\Pi_{k}f_{m}\\wedge_{k}f_{m} cross-endroy\\overline{cos\\theta_{S}}=arctan\\varphi_{P}\\varphi_{P}\\overline{\\varphi} loss canlog_{S}tan be\\mathbb{Z}\\newline\nwritings.\\overline{|uncitive}\\newline\nSPECIAL_CHARACTER\\overline{L(y,\\hat{y})}= -Eyi.logy-\\sum^{i}y_{i}\\cdot log\\hat{y}_{i}\\newline\ni-1\\tilde{t}_{1}\\newline\n\\\\newpage\n    \\end{verbatim}\n    \\end{document}\nLaTeX compilation success:\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**code using latest cv2 version to detect words in an image**"
      ],
      "metadata": {
        "id": "z3nuEv2OFyMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cropped_images = []\n",
        "special_character_images = []\n",
        "recognized_texts = []\n",
        "def upload_pdf(pdf_path):\n",
        "    _, ext = os.path.splitext(pdf_path)\n",
        "    ext = ext.lower()\n",
        "    if ext == \".pdf\":\n",
        "      images = convert_from_path(pdf_path, dpi=200, fmt=\"jpeg\")\n",
        "    elif ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\"]:\n",
        "      images = [cv2.imread(pdf_path)]\n",
        "    print(\"setp 2\")\n",
        "    # Save images to the current working directory and collect their paths\n",
        "    image_paths = []\n",
        "    merged_paragraphs = []\n",
        "     #  initialize empty list\n",
        "    full_text=\"\"\n",
        "    for i, image in enumerate(images):\n",
        "\n",
        "        print(\"setp 3\")\n",
        "        if ext == \".pdf\":\n",
        "            filename = os.path.join('/kaggle/working/', f'converted_image_{i + 1}.jpeg')\n",
        "            image.save(filename, \"JPEG\")\n",
        "            img = cv2.imread(filename)\n",
        "            image=img\n",
        "\n",
        "             #  Creating a copy of the image to make changes\n",
        "            grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  #  Converting to greyscale\n",
        "            # Apply Otsu filtering\n",
        "            otsu1 = cv2.threshold(grey, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)[1]\n",
        "#             horz_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (14, 1))\n",
        "#             # The above line creates a rectangular kernel of dimension (14, 1)\n",
        "#             lines = cv2.morphologyEx(otsu1, cv2.MORPH_OPEN, horz_kernel, iterations=2)\n",
        "\n",
        "#             cnts = cv2.findContours(lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            # findContours returns the contours and the hierarchy for opencv version 4.0 and later\n",
        "            # earlier versions return: [image, contours, hierarchy]\n",
        "#             cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
        "#             for c in cnts:\n",
        "#                 cv2.drawContours(img, [c], -1, (255, 255, 255), 2)\n",
        "            grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            otsu2 = cv2.threshold(grey, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)[1]\n",
        "            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (64,9))\n",
        "            dilation = cv2.dilate(otsu2, kernel, iterations = 1)\n",
        "            contours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL,\n",
        "                                        cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "            count = 0  #  keep a count for the words (optional)\n",
        "\n",
        "\n",
        "            y_positions=[]\n",
        "            for c in contours:\n",
        "                x, y, w, h = cv2.boundingRect(c)  #  get coordinates of contouts\n",
        "                if w>50 and h>25:  #  threshold for size of bounding box\n",
        "                    count += 1\n",
        "                    cropped = image[y:y+h, x:x+w]  # Crop the image\n",
        "\n",
        "                    # Calculate scale to fit the larger side into 224 while keeping aspect ratio\n",
        "                    scale = min(224 / w, 224 / h)\n",
        "                    new_w, new_h = int(w * scale), int(h * scale)\n",
        "\n",
        "                    # Resize the cropped image\n",
        "                    resized = cv2.resize(cropped, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "                    # Create a 224x224 white background\n",
        "                    padded = np.ones((224, 224, 3), dtype=np.uint8) * 255 if len(resized.shape) == 3 else np.ones((224, 224), dtype=np.uint8) * 255\n",
        "\n",
        "                    # Calculate padding to center the image\n",
        "                    x_offset = (224 - new_w) // 2\n",
        "                    y_offset = (224 - new_h) // 2\n",
        "\n",
        "                    # Place the resized image on the white background\n",
        "                    padded[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized\n",
        "\n",
        "                    # Append to the list\n",
        "                    cropped_images.append(padded)\n",
        "            #         cropped_images.append(cv2.resize(otsu2[y:h+y, x:w+x], (224, 224))) # Resizing to fixed size\n",
        "#                     rect = cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2) # Draw Bounding boxes\n",
        "                    y_positions.append(np.mean([y,y+h]))\n",
        "\n",
        "            print(count, \" words extracted!\")     # Recognize text from cropped images\n",
        "            recognized_texts = recognize_text_from_images(cropped_images)\n",
        "            print(len(recognized_texts))\n",
        "    #         recognized_texts = [\"sorry will be soon available\",\"facing issues with resources\"]\n",
        "            # Combine recognized texts into paragraphs based on y-positions\n",
        "\n",
        "            current_paragraph = \"\"\n",
        "            threshold = 70  # Define a threshold for merging based on y-coordinate proximity\n",
        "\n",
        "            for i, text in enumerate(recognized_texts):\n",
        "                if text.strip():  # If the recognized text is not empty\n",
        "                    if current_paragraph:  # If there is already some text in the paragraph\n",
        "                        # Check if current and last y-position are within the threshold\n",
        "                        if abs(y_positions[i] - y_positions[i - 1]) < threshold:\n",
        "                            current_paragraph=  current_paragraph + text.strip()+ \" \"\n",
        "                        else:\n",
        "                            merged_paragraphs.append(current_paragraph.strip())\n",
        "                            current_paragraph = text.strip()+ \" \"   # Start a new paragraph\n",
        "                    else:\n",
        "                        current_paragraph = text.strip()+ \" \"   # Start a new paragraph\n",
        "\n",
        "            # Append any remaining text\n",
        "            if current_paragraph:\n",
        "                merged_paragraphs.append(current_paragraph.strip())\n",
        "        else:\n",
        "            cropped_img=image\n",
        "            orig_h, orig_w = cropped_img.shape[:2]\n",
        "\n",
        "            # Scale the image to fit within 224x224 while keeping the aspect ratio\n",
        "            scale = min(224 / orig_w, 224 / orig_h)\n",
        "            new_w, new_h = int(orig_w * scale), int(orig_h * scale)\n",
        "\n",
        "            # Resize the cropped image\n",
        "            resized = cv2.resize(cropped_img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            # Create a 224x224 white background with 3 channels if resized is in color\n",
        "            padded = np.ones((224, 224, 3), dtype=np.uint8) * 255 if len(resized.shape) == 3 else np.ones((224, 224), dtype=np.uint8) * 255\n",
        "\n",
        "            # Calculate padding to center the resized image\n",
        "            x_offset = (224 - new_w) // 2\n",
        "            y_offset = (224 - new_h) // 2\n",
        "\n",
        "            # Place the resized image on the white background\n",
        "            padded[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized\n",
        "\n",
        "            # Append to the list of cropped images\n",
        "            cropped_images.append(padded)\n",
        "\n",
        "            # Recognize text from the padded image and store in merged_paragraphs\n",
        "            merged_paragraphs.append(recognize_text_from_images([padded]))\n",
        "\n",
        "        # Print the recognized text paragraphs\n",
        "        print(\"Recognized Text Paragraphs:\")\n",
        "        for i, paragraph in enumerate(merged_paragraphs):\n",
        "            full_text+=f\"\"\"{paragraph}\n",
        "\"\"\"\n",
        "        full_text+= r\"\"\" \"\"\"\n",
        "\n",
        "    # Minimal LaTeX code for testing\n",
        "        # Minimal LaTeX code for testing\n",
        "    header = latex_code = r\"\"\"\n",
        "\\documentclass{article}\n",
        "\\usepackage{amsmath}\n",
        "\n",
        "\\begin{document}\n",
        "\n",
        "\\title{Digital Document}\n",
        "\\author{IE 643}\n",
        "\\date{\\today}\n",
        "\\maketitle\n",
        "\n",
        "\\begin{verbatim}\n",
        "\"\"\"\n",
        "    footer=r\"\"\"\n",
        "    \\end{verbatim}\n",
        "    \\end{document}\"\"\"\n",
        "\n",
        "    latex_code = header+full_text+footer\n",
        "    print(latex_code)\n",
        "    # Write LaTeX code to file in the current working directory\n",
        "    latex_file_path = os.path.join(os.getcwd(), \"output.tex\")\n",
        "    with open(latex_file_path, \"w\") as f:\n",
        "        f.write(latex_code)\n",
        "\n",
        "    # Compile the LaTeX file\n",
        "    process = subprocess.run(['pdflatex', latex_file_path], capture_output=True, text=True)\n",
        "\n",
        "    if process.returncode != 0:\n",
        "        print(\"LaTeX compilation failed:\")\n",
        "        print(process.stderr)\n",
        "\n",
        "    else:\n",
        "        print(\"LaTeX compilation success:\")\n",
        "        pdf_output_path = os.path.join(os.getcwd(), \"output.pdf\")\n",
        "\n",
        "\n",
        "\n",
        "upload_pdf(\"/kaggle/input/mathdocument/handwritten_mathematical.pdf\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-26T11:14:25.57565Z",
          "iopub.execute_input": "2024-11-26T11:14:25.575974Z",
          "iopub.status.idle": "2024-11-26T11:14:28.712296Z",
          "shell.execute_reply.started": "2024-11-26T11:14:25.575943Z",
          "shell.execute_reply": "2024-11-26T11:14:28.71119Z"
        },
        "trusted": true,
        "id": "F12pcwG8FyMU",
        "outputId": "a108807c-55fc-432e-9b32-e75cd072a6f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "setp 2\nsetp 3\n10  words extracted!\n10\nRecognized Text Paragraphs:\n\n\\documentclass{article}\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\title{Digital Document}\n\\author{IE 643}\n\\date{\\today}\n\\maketitle\n\n\\begin{verbatim}\n\\tilde{t}_{1}\nL(y,\\hat{y})= -\\overline{\\zeta}^{1} \\hat{y}_{i}\\cdotlog\\hat{y}_{i}\n\\hat{a}_{a}\nuncible_{n}a_{3}:\n\\tilde{\\gamma} \\pi_{e}f_{sim}^{f_{sim}} cos-entropy^{log}cosan \\mathbb{Z}\n \n    \\end{verbatim}\n    \\end{document}\nLaTeX compilation success:\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "*end of the code*\n"
      ],
      "metadata": {
        "id": "hXxfbnLLFyMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(cropped_images)"
      ],
      "metadata": {
        "trusted": true,
        "id": "lxt7epkrFyMW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define grid size for displaying all 162 images\n",
        "num_images = len(special_character_images)\n",
        "rows = 9\n",
        "cols = 18\n",
        "\n",
        "# Set up the figure size to accommodate the grid\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "# Loop through each cropped and padded image\n",
        "for i in range(num_images):\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "    plt.imshow(special_character_images[i],cmap='gray')  # Display each image in grayscale\n",
        "\n",
        "    plt.axis('off')  # Turn off axis for a cleaner look\n",
        "\n",
        "# Adjust layout to minimize gaps between subplots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "6REhBZ6YFyMW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define grid size for displaying all 162 images\n",
        "num_images = len(cropped_images)\n",
        "rows = 9\n",
        "cols = 18\n",
        "\n",
        "# Set up the figure size to accommodate the grid\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "# Loop through each cropped and padded image\n",
        "for i in range(num_images):\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "    plt.imshow(cropped_images[i])  # Display each image in grayscale\n",
        "    plt.title(recognized_texts[i])\n",
        "    plt.axis('off')  # Turn off axis for a cleaner look\n",
        "\n",
        "# Adjust layout to minimize gaps between subplots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "yzsw9RDVFyMW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "ax = plt.subplots(2,3, figsize=(12,7))[1]\n",
        "ax[0,0].imshow(image)\n",
        "ax[0,0].set_title('Image')\n",
        "\n",
        "ax[0,2].imshow(img)\n",
        "ax[0,2].set_title('Lines Removed')\n",
        "ax[1,0].imshow(otsu2)\n",
        "ax[1,0].set_title('Re-Otsuing')\n",
        "ax[1,1].imshow(dilation)\n",
        "ax[1,1].set_title('Dilated')\n",
        "ax[1,2].imshow(image)\n",
        "ax[1,2].set_title('Result')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "cW18U2_9FyMW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the final result in a larger size\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image)\n",
        "plt.title('Final Result (Larger View)')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "w2jK72WEFyMX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(r\"/kaggle/input/dataset-model/20241107_185154.jpg\")\n",
        "image = img.copy()  #  Creating a copy of the image to make changes\n",
        "grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  #  Converting to greyscale\n",
        "# Apply Otsu filtering\n",
        "otsu1 = cv2.threshold(grey, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)[1]\n",
        "horz_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (14, 1))\n",
        "# The above line creates a rectangular kernel of dimension (14, 1)\n",
        "lines = cv2.morphologyEx(otsu1, cv2.MORPH_OPEN, horz_kernel, iterations=2)\n",
        "\n",
        "cnts = cv2.findContours(lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "# findContours returns the contours and the hierarchy for opencv version 4.0 and later\n",
        "# earlier versions return: [image, contours, hierarchy]\n",
        "cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
        "for c in cnts:\n",
        "    cv2.drawContours(img, [c], -1, (255, 255, 255), 2)\n",
        "grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "otsu2 = cv2.threshold(grey, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)[1]\n",
        "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (14,9))\n",
        "dilation = cv2.dilate(otsu2, kernel, iterations = 1)\n",
        "contours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL,\n",
        "                            cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "img_list = []  #  initialize empty list\n",
        "count = 0  #  keep a count for the words (optional)\n",
        "cropped_images = []  #  initialize empty list\n",
        "count = 0  #  keep a count for the words (optional)\n",
        "y_positions=[]\n",
        "for c in contours:\n",
        "    x, y, w, h = cv2.boundingRect(c)  #  get coordinates of contouts\n",
        "    if w>50 and h>25:  #  threshold for size of bounding box\n",
        "        count += 1\n",
        "        cropped = otsu2[y:y+h, x:x+w]  # Crop the image\n",
        "\n",
        "        # Calculate scale to fit the larger side into 224 while keeping aspect ratio\n",
        "        scale = min(224 / w, 224 / h)\n",
        "        new_w, new_h = int(w * scale), int(h * scale)\n",
        "\n",
        "        # Resize the cropped image\n",
        "        resized = cv2.resize(cropped, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # Create a 224x224 white background\n",
        "        padded = np.ones((224, 224), dtype=np.uint8) * 255\n",
        "\n",
        "        # Calculate padding to center the image\n",
        "        x_offset = (224 - new_w) // 2\n",
        "        y_offset = (224 - new_h) // 2\n",
        "\n",
        "        # Place the resized image on the white background\n",
        "        padded[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized\n",
        "\n",
        "        # Append to the list\n",
        "        cropped_images.append(padded)\n",
        "#         cropped_images.append(cv2.resize(otsu2[y:h+y, x:w+x], (224, 224))) # Resizing to fixed size\n",
        "        rect = cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2) # Draw Bounding boxes\n",
        "        y_positions.append(np.mean([y,y+h]))\n",
        "\n",
        "print(count, \" words extracted!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "S0RtsQIFFyMX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "U1wtQX9fFyMX"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
